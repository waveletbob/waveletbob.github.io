---
layout: post
title: SVM（Support Vector Machine）
categories: 机器学习与数据挖掘
tags: 机器学习

---


### 1）Linear可分SVM （硬间隔最大化）###

找分离超平面：**w**·x+**b**，目标线：希望能够容忍数据最多的测量误差（容忍度）

函数间隔和几何间隔

间隔最大化，误差最小化

最大间隔分割超平面存在的唯一性证明

![](https://i.imgur.com/VZTsy7t.png)


### 2）线性支持向量机 （软间隔最大化，近似可分）###

加一个松弛变量，惩罚因子

![](https://i.imgur.com/6NWBeZD.png)

### 3）非线性支持向量机（核方法技巧） ###

核函数：

- 线性核函数**K(x,y)=x·y**；
- 多项式核函数**K(x,y)=[(x·y)+1]^d**；
- 径向基函数**K(x,y)=exp(-|x-y|^2/d^2）**；（高斯）
- Sigmoid二层神经网络核函数**K(x,y)=tanh(a(x·y)+b）**

在高维转化时，由于高维很难理解，且可以分解为矩阵列乘的形式，因此将φ（X，Y）=K（X,Y）

### 4）总结 ###

分类超平面只与支持向量有关，数据量大，SVM很复杂，适合数据量小的情况（大数据下，如何利用SVM？）

