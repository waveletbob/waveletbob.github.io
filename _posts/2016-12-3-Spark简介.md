---
layout: post
title: Spark简介
categories: 数据科学
tags: Spark

---


### 0）基础 ###

- 基本知识
	- Hadoop简介
	- map-reduce的原理、开发、工作机制、作业、输入输出类型格式
	- hdfs
	- yarn
	- hadoop IO
	- map-reduce特性
	- 开源项目
	- 集群构建与管理
- **map-reduce模型**

**首先**，map task会从本地文件系统读取数据并以key-value的键值对形式集合，使用的是hadoop内置数据类型（当然也可以自定义数据类型）？

答：首先是序列化和反序列化的概念，序列化在分布式数据处理领域常常出现：进程通讯（RPC：将消息二进制流化传输并反序列化）、永久存储。因此Hadoop需要使用自己的序列化格式Writable，它格式紧凑、速度快。

hadoop的数据类型实现Writable接口

	实现write（）、readFields（），

	如果是作为主键Key使用需要实现WritableComparable接口实现其write(),readFields(),CompareTo() 方法

	数据类型，必须要有一个无参的构造方法，为了方便反射，进行创建对象。

	在自定义数据类型中，建议使用java的原生数据类型，最好不要使用Hadoop对原生类型进行封装的数据类型。

便于序列化进行网络传输和文件传输。

数据的输入输出格式与其自定义：

（InputFormat） 用于描述MapReduce作业的数据输入规范。数据输入格式完成输入规范检查(比如输入文件目录的检查)、对数据文件进行输入分块(也叫分片，InputSplit)，以及提供从输入分块(分片)中将数据记录逐一读出，并转化为Map过程的输入键值对等功能。

TextInputFormat和KeyValueInputFormat、TextInputFormat不同的格式定义不同的方式分割文件以及生成K-V键值对，该功能由RecordReader负责，相应的，OutputFormat中以RecordWriter负责写入数据格式。

实现自定义输入输出格式时只需要继承相应的类以及实现其方法即可。

**转换成key-value之后**，有一个**partition**的分区操作过程，分区按照Key进行划分，默认使用hashpartiitoner，可以重写getpartition方法来自定义规则，

**之后对key进行sort排序**，grouping分组将相同key的value合并分组输出，

之后利用**combiner**进行本地规约，减小后续**shuffle和reduce**工作量

之后按照输出数据格式进行结果写入输出。

- **shuffle理解与调优**

shuffle是map输出作为输入到reducer的按键排序中间阶段，分为partition、sort、combiner

调优：
	map端（减小中间结果或者读取磁盘的频率以及后续的工作压力）：
			- io.sort.mb(spill)
			- io.sort.spill.percent(spill)
			- min.num.spill.for.combine(combiner)
			- mapred.compress.map.output(压缩)
			- mapred.map.output.compression.codec(压缩格式)
	reducer端
			- mapred.reduce.parallel.copies（并行度）
			- mapred.reduce.copy.backoff（网络环境不好的话可以调整）
			- io.sort.factor（加大merge的行为）
			- mapred.job.shuffle.merge.percent（下载快容易把内存撑大）
			- mapred.job.reduce.input.buffer.percent（分内存来缓存）

- Hadoop相关进程角色
	- Namenode：它运行上Master节点上，负责存储的文件和目录所有元数据。它管理文件的块信息，以及块在集群中分布的信息。
	- Datanode：它是一个存储实际数据的Slave节点。它定时向Namenode发送本节点上块的信息。
	- Secondary Namenode：它会定期通过Editlog合并NameNode的变化，从而它r的日志不会过大。它可以在NameNode的故障的情况下做为副本使用。
	- JobTracker：这是运行在Namenode上，负责提交和跟踪MapReduce Job的守护程序。它会向Tasktracker分配的任务。
	- TaskTracker：这是Datanode上运行的守护进程。它在Slave节点上负责具体任务的运行。
	- ResourceManager的（Hadoop的2.X）：它负责YARN上运行的资源和调度。
	- NodeManager（Hadoop的2.X）：它可以运行在Slave节点，并负责启动应用程序的容器，监测他们的资源使用情况（CPU，内存，磁盘，网络），并报告这些到ResourceManager。
	- JobHistoryServer（Hadoop的2.X）：它维护有关的MapReduce工作中的应用终止后的信息。
- Hadoop1.X与2.x的区别
	2在1的基础之上解决了1中存在的单点故障问题，主被动namenode，主动挂了，被动负责接锅，高可用；

	另外，有个YARN可以允许执行多个程序，中央资源管理器。

	1默认数据块大小：64MB，2是128MB
- 如何删除和添加hadoop集群节点
- 拒绝相同文件多个客户端访问（加了个租约）
- 机架感知
	副本策略：2个在同一机架内，另外一个在其他机架，确定块放置方式
- hadoop适用于大型数据集、不适用大量小文件（元数据信息会撑爆namenode）

- Spark程序执行流程、shuffle原理与数据倾斜调优（预处理、过滤key、增加并行度、聚合、reduce join->map join ）、OOM内存溢出、cache与persist缓存比较、

- Spark结构化数据处理：Spark SQL、DataFrame和Dataset（RDD-DataFrame-Dataset）

- Spark非结构化数据处理：利用filter等进行数据的处理

- Spark性能优化主要有哪些手段？

	- 开发调优
		- 避免创建重复的RDD
		- 尽可能复用同一个RDD
		- 对多次使用的RDD进行持久化
		- 尽量避免使用shuffle类算子（reducebykey、join、distinct、repartition）
		- 使用map-side预聚合的shuffle操作
		- 使用改性能算子
			- reduceByKey/aggregateByKey替代groupByKey
			- mapPartitions替代普通map
			- foreachPartitions替代foreach
			- filter之后进行coalesce操作
			- repartitionAndSortWithinPartitions替代repartition与sort类操作
		- 广播大变量
		- 使用Kryo优化序列化性能
		- 优化数据结构
	- 资源调优
	- 数据倾斜调优
	- shuffle调优
- Spark优缺点
- 

- **TextInputFormat作用，如何自定义**

首先InputFormat会对数据进行两方面的预处理：

	getSplits对数据进行分片；
	getRecordReader对每个分片进行Key-Value键值对格式传递给map
TextInputFormat将行偏移量作为key，行内容作为value。

自定义：继承InputFormat接口，重写createRecordReader和isSplitable方法 ，其中createRecordReader可以自定义分隔符。

- **Hadoop与Spark并行计算比较**

- **hdfs架构**

当hdfs上的文件达到一定大小会形成一个文件，或者超过一段时间形成一个文件；

文件都存储在datanode上，namenode记录datanode的元数据信息，而且元数据信息存储在内存中，所以当文件切片很小或很多的时候会卡死。

- **map-reduce程序运行会有什么比较常见的问题**


作业大部分做完了，但还是有几个reduce一直在运行？

这是因为这几个reduce要处理的数据远大于其他的reduce，可能是对键值对任务的不平均划分引起的数据倾斜，

解决办法分区时重新定义规则，进行拆分、打散等操作，或者在map端的combiner端进行数据预处理。

- **hadoop和spark的shuffle过程**

hadoop：map保存分片数据、通过网络收集到reduce端

spark：spark是DAGSchedular划分Stage的时候产生的，TaskScheduler分发Stage到各个Worker的Executor减少shuffle提高性能

- **Hive存放的是什么**

逻辑上的数据仓库，存的是hdfs的映射，实际操作的都是hdfs的文件，HQL就是用sql语法来写的mr程序。

- **Hive与关系型数据库的关系**

没有关系，hive是数据仓库，不能和数据库一样进行实时的CURD操作，一次写入多次读取的操作，是ETL工具

- **Flume工作机制是什么**

核心概念是agent，里面包括source、chanel、sink三个组件

source运行在日志收集节点进行日志采集，之后临时存储在chanel中，sink负责将chanel中的数据发送到目的地。 
只有成功发送之后chanel中的数据才会被删除。 

首先书写flume配置文件，定义agent、source、chanel和sink然后将其组装，执行flume-ng命令。

- **Sqoop工作原理**

hadoop生态圈上的数据传输工具。 
可以将关系型数据库的数据导入非结构化的hdfs、hive或者bbase中，也可以将hdfs中的数据导出到关系型数据库或者文本文件中。 使用的是mr程序来执行任务，使用jdbc和关系型数据库进行交互。 

import原理：通过指定的分隔符进行数据切分，将分片传入各个map中，在map任务中在每行数据进行写入处理没有reduce。 

export原理：根据要操作的表名生成一个java类，并读取其元数据信息和分隔符对非结构化的数据进行匹配，多个map作业同时执行写入关系型数据库

- **Hbase行健列族概念、物理模型、表设计原则**

行健：是hbase表自带的，每个行健对应一条数据。 

列族：是创建表时指定的，为列的集合，每个列族作为一个文件单独存储，存储的数据都是字节数组，其中的数据可以有很多，通过时间戳来区分。 

物理模型：整个hbase表会拆分为多个region，每个region记录着行健的起始点保存在不同的节点上，查询时就是对各个节点的并行查询，当region很大时使用.META表存储各个region的起始点，-ROOT又可以存储.META的起始点。 

rowkey的设计原则：各个列簇数据平衡，长度原则、相邻原则，创建表的时候设置表放入regionserver缓存中，避免自动增长和时间，使用字节数组代替string，最大长度64kb，最好16字节以内，按天分表，两个字节散列，四个字节存储时分毫秒。 

列族的设计原则：尽可能少（按照列族进行存储，按照region进行读取，不必要的io操作），经常和不经常使用的两类数据放入不同列族中，列族名字尽可能短。

- **Spark Streaming 和 Storm区别**

- **Hadoop性能调优**

系统配置、程序编写、作业调度（三个角度）

 hdfs.block.size=128/256(默认64)

mapred.map.tasks、
mapred.reduce.tasks、

mapred.tasktracker.map.tasks.maximum每台机器上的最大map任务数 

mapred.tasktracker.reduce.tasks.maximum每台机器上的最大reduce任务数 

mapred.reduce.slowstart.completed.maps配置reduce任务在map任务完成到百分之几的时候开始进入 

mapred.compress.map.output,mapred.output.compress配置压缩项，消耗cpu提升网络和磁盘io 

合理利用combiner 

注意重用writable对象

- **hadoop高并发**

首先保证集群的高可靠性，高并发不会挂掉，可以横向扩展，挂掉了重启

- **RDD**
- **spark组件**
- **spark工作机制**
- **spark优化**
- **kafka工作原理**

### 1）基本概念 ###

- SparkSession
- DataFrames（与R、Python等对接）
- Partitions（数据并行化）
- Transformations（改变DataFrames）
- Lazy Evaluation（等待一段时间再运行Tranformations）
- Actions（从Transformations计算结果）
- Spark UI（4040）

### 2）一个基础的数据流转换 ###

以航空数据为例子进行操作：


读取json数据生成DF：

	val flightData2015 = spark.read.json("/mnt/defg/chapter-1-data/json/2015-summary.json")
	flightData2015.take(2)
	flightData2015.explain()

	//Tranformation
	val sortedFlightData2015 = flightData2015.sort("count")
	sortedFlightData2015.explain()
	sortedFlightData2015.take(2)

	val flightData2015 = spark.read.option("inferSchema", "true").option("header", "true").csv("/mnt/defg/chapter-1-data/csv/2015-summary.csv")

### 3）DataFrames and SQL ###

	flightData2015.createOrReplaceTempView("flight_data_2015")
	val sqlWay = spark.sql("""
	SELECT DEST_COUNTRY_NAME, count(1)
	FROM flight_data_2015
	GROUP BY DEST_COUNTRY_NAME
	""")
	val dataFrameWay = flightData2015
	.groupBy('DEST_COUNTRY_NAME)
	.count()

	spark.sql("SELECT max(count) from flight_data_2015").take(1)
	import org.apache.spark.sql.functions.max
	flightData2015.select(max("count")).take(1)

	//Top5
	val maxSql = spark.sql("""
	SELECT DEST_COUNTRY_NAME, sum(count) as destination_total
	FROM flight_data_2015
	GROUP BY DEST_COUNTRY_NAME
	ORDER BY sum(count) DESC
	LIMIT 5""")
	maxSql.collect()

	flightData2015
	.groupBy("DEST_COUNTRY_NAME")
	.sum("count")
	.withColumnRenamed("sum(count)", "destination_total")
	.sort(desc("destination_total"))
	.limit(5)
	.collect()

### 4）Structed APIs ###

Spark’s Structured APIs allow for transformations and actions on structured and semi-structured data.

**DataFrames, Datasets,and in Spark SQL**

Spark使用transformation和action来构建程序数据流，一个job分成stages和tasks进行集群运行，通常使用dataframes和datasets来存储数据，其中transformation生成DF和DS，action开启计算任务。

 -  **DataFrames and Datasets**

tables with rows and columns.

 -  **schema**.

A schema defines the column names and types
of a DataFrame.

	val df = spark.range(500).toDF("number")
	df.select(df.col("number") + 10)
	
分为两类Structed APIS：DataFrame or “untyped API”；“typed API” or “Dataset API”

- **Columns**

- **Rows**

spark导入导出数据

	spark.range(2).toDF().collect()
	
- **Spark Value Types**

- **Encoders**

定义自己的类型并在Rows中使用

Encoders are only available in Scala, with case classes, and Java,with JavaBeans.

- **Spark Execution**

一个用户查询代码从写到运行的过程：

1. Write DataFrame/Dataset/SQL Code
2. If valid code, Spark converts this to a Logical Plan
3. Spark transforms this Logical Plan to a Physical Plan
4. Spark then executes this Physical Plan on the cluster

Catalyst Optimizer优化器解析代码转换后成Spark代码

- **Logical Planning**

UserCode->unresolved logical plan->catalog->analyzer->resolved logical plan->optimizer->optimized logical plan

- **Physical Planning**

### 5）Basic Structured Operations ###

- **Schema**




