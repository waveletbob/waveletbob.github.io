---
layout: post
title: 决策树算法
categories: 机器学习与数据挖掘
tags: 机器学习

---

### 1）信息论概念 ###

- 熵与互信息
熵：平均不确定性的度量

平均互信息：得知特征Y的信息而使对X信息的不确定性减少的程度

决策树：自顶向下、熵值下降的树、叶子处熵为零、有监督学习

- 信息增益（互信息=熵-条件熵）
- 信息增益率（互信息/熵）
- 基尼指数

### 2）算法 ###

- ID3（根据信息增益进行树的构建）

![](http://img.blog.csdn.net/20131027123630593?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRGFya19TY29wZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)


![](https://i.imgur.com/zH8UhQh.png)

- C4.5（将ID3中的信息增益改为信息增益比）

在ID3基础之上（特征分类很细）对其进行惩罚，防止过拟合，克服泛化能力（未知数据上的表现）不够强的缺点

![](http://img.blog.csdn.net/20131027124155359?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRGFya19TY29wZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

- CART（用到的是基尼系数）
![](http://img.blog.csdn.net/20131027125134515?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRGFya19TY29wZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

### 3）实现 ###

	from math import log
	import operator
	dataset,label=createDataSet()
	# calcShannonEnt(dataset)
	# chooseBestFeatureToSplit(dataset)
	# print(createTree(dataset,labels))
	def createDataSet():
	    dataSet = [[1, 1, 'yes'],
	               [1, 1, 'yes'],
	               [1, 0, 'no'],
	               [0, 1, 'no'],
	               [0, 1, 'no']]
	    labels = ['no surfacing','flippers']
	    #change to discrete values
	    return dataSet, labels
	
	def calcShannonEnt(dataSet):
	    numEntries = len(dataSet)
	    labelCounts = {}
	    for featVec in dataSet: #the the number of unique elements and their occurance
	        currentLabel = featVec[-1]
	        if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0
	        labelCounts[currentLabel] += 1
	    shannonEnt = 0.0
	    for key in labelCounts:
	        prob = float(labelCounts[key])/numEntries
	        shannonEnt -= prob * log(prob,2) #log base 2
	    return shannonEnt
	    
	def splitDataSet(dataSet, axis, value):
	    retDataSet = []
	    for featVec in dataSet:
	        if featVec[axis] == value:
	            reducedFeatVec = featVec[:axis]     #chop out axis used for splitting
	            reducedFeatVec.extend(featVec[axis+1:])
	            retDataSet.append(reducedFeatVec)
	    return retDataSet
	    
	def chooseBestFeatureToSplit(dataSet):
	    numFeatures = len(dataSet[0]) - 1      #the last column is used for the labels
	    baseEntropy = calcShannonEnt(dataSet)
	    bestInfoGain = 0.0; bestFeature = -1
	    for i in range(numFeatures):        #iterate over all the features
	        featList = [example[i] for example in dataSet]#create a list of all the examples of this feature
	        uniqueVals = set(featList)       #get a set of unique values
	        newEntropy = 0.0
	        for value in uniqueVals:
	            subDataSet = splitDataSet(dataSet, i, value)
	            prob = len(subDataSet)/float(len(dataSet))
	            newEntropy += prob * calcShannonEnt(subDataSet)     
	        infoGain = baseEntropy - newEntropy     #calculate the info gain; ie reduction in entropy
	        if (infoGain > bestInfoGain):       #compare this to the best gain so far
	            bestInfoGain = infoGain         #if better than current best, set to best
	            bestFeature = i
	    return bestFeature                      #returns an integer
	
	def majorityCnt(classList):
	    classCount={}
	    for vote in classList:
	        if vote not in classCount.keys(): classCount[vote] = 0
	        classCount[vote] += 1
	    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)
	    return sortedClassCount[0][0]
	
	def createTree(dataSet,labels):
	    classList = [example[-1] for example in dataSet]
	    if classList.count(classList[0]) == len(classList): 
	        return classList[0]#stop splitting when all of the classes are equal
	    if len(dataSet[0]) == 1: #stop splitting when there are no more features in dataSet
	        return majorityCnt(classList)
	    bestFeat = chooseBestFeatureToSplit(dataSet)
	    bestFeatLabel = labels[bestFeat]
	    myTree = {bestFeatLabel:{}}
	    del(labels[bestFeat])
	    featValues = [example[bestFeat] for example in dataSet]
	    uniqueVals = set(featValues)
	    for value in uniqueVals:
	        subLabels = labels[:]       #copy all of labels, so trees don't mess up existing labels
	        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)
	    return myTree                            
	    
	def classify(inputTree,featLabels,testVec):
	    firstStr = inputTree.keys()[0]
	    secondDict = inputTree[firstStr]
	    featIndex = featLabels.index(firstStr)
	    key = testVec[featIndex]
	    valueOfFeat = secondDict[key]
	    if isinstance(valueOfFeat, dict): 
	        classLabel = classify(valueOfFeat, featLabels, testVec)
	    else: classLabel = valueOfFeat
	    return classLabel
	
	def storeTree(inputTree,filename):
	    import pickle
	    fw = open(filename,'w')
	    pickle.dump(inputTree,fw)
	    fw.close()
	    
	def grabTree(filename):
	    import pickle
	    fr = open(filename)
	    return pickle.load(fr)


### 4）Bagging与Random Forest（RF） ###

bootstraping有放回采样

bagging多个采样分类器取最好的（如果多分类器全是决策树就是随机森林否则就是普通bagging）

RF：重采样本n个、随机K（一般对总数去对数lg）个属性建立CART、通过投票表决定哪一类，对样本有权重分类器没有，泛化能力强

### 5）提升：Adaboost/GBDT ###

g（分类器）、α（分类器权重）、最后的分类器为权重和

误分类样本权重w增加得出新的分类器g，同时也能得到分类器的权重α

与RF区别，RF只有样本权重，而Adaboost有样本权重和分类器权重（同时得到），随机森林取最好的，Adaboost取权重和。在Adaboost中，后面得到的分类器权重α应该比前面得到的分类器权重低。

GBDT是Adaboost的特殊情况

### 6）Xgboost ###

1、CART：先对特征排序，然后按照Gini系数分裂（最小的划分），基于单特征，所以没有考虑特征间的关系，数据不需要归一化。

2、boosting tree

3、GBDT

4、XGBoost

相关资料：XGBoost算法详解[CV Download](https://raw.githubusercontent.com/waveletbob/waveletbob.github.io/master/image/XGBoost - Kesci.pdf)








